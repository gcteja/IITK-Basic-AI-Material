{\rtf1\ansi\ansicpg1252\cocoartf2822
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;\red109\green109\blue109;}
{\*\expandedcolortbl;;\cssrgb\c0\c0\c0;\cssrgb\c50196\c50196\c50196;}
{\*\listtable{\list\listtemplateid1\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid1\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid1}
{\list\listtemplateid2\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid101\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid2}
{\list\listtemplateid3\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid201\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid3}
{\list\listtemplateid4\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid301\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid4}
{\list\listtemplateid5\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid401\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid5}}
{\*\listoverridetable{\listoverride\listid1\listoverridecount0\ls1}{\listoverride\listid2\listoverridecount0\ls2}{\listoverride\listid3\listoverridecount0\ls3}{\listoverride\listid4\listoverridecount0\ls4}{\listoverride\listid5\listoverridecount0\ls5}}
\paperw11900\paperh16840\margl1440\margr1440\vieww29920\viewh15660\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 ------------------------------------------------------------------------------------------\
\
\
COURSE: Advanced AI\
TOPIC: Overview of key models: GPT, BERT,T5, LLaMA, Falcon.\
DUE-DATE: 25 Jan 2025\
\
\
INSTRUCTIONS:\
1. Do not modify any tag in this file. Just type your answers in the designated place.\
2. Do not change the file name and/or extension.\
3. Upload this file after answering in the portal.\
\
\
------------------------------------------------------------------------------------------\
\
\
<<< QUESTION 1 >>>\
\
\
What is the additional number of parameters that you will need apart from the pretrained weights in the following scenarios? (a) Text classification (3 classes) using BERT-base, (b) Text Summarization using T5.\
\
\
### WRITE ANSWER BELOW ###\
\
Additional parameters beyond pretrained weights:\
\
(a) Text classification (3 classes) using BERT-base\
\
BERT-base has a hidden size of 768. For a 3-class classification task, we add a simple linear classification layer on top of the [CLS] token.\
This layer has:\
\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls1\ilvl0\cf0 {\listtext	\uc0\u8226 	}768\'d73=2304 weight parameters\
{\listtext	\uc0\u8226 	}3 bias parameters\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
So, the total additional parameters required are 2,307.\
\
\pard\pardeftab720\sa240\partightenfactor0
\cf0 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 (b) Text Summarization using T5\
T5 is pretrained as a text-to-text (encoder\'96decoder) model, and summarization is already part of its pretraining objective. Therefore, no extra task-specific head is needed.\
So, the additional number of parameters required is 0.\kerning1\expnd0\expndtw0 \outl0\strokewidth0 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
<<< QUESTION 2 >>>\
\
\
Consider a task where you would have a paragraph and a question as input, and you need to generate the answer. Which pretrained model would you use among BERT, GPT and T5? What are the advantages and disadvantages of the allowed architectures in terms of (a) number of parameters to be fine-tuned and (b) representation capacity in the context of this problem?  Can you think of a method that can take advantage of each of the allowed architectures? [Assume that all these models have the same model dimension and number of layers]\
\
\
### WRITE ANSWER BELOW ###\
\
\
\pard\pardeftab720\sa240\partightenfactor0
\cf0 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 For this task, the best choice is T5 as the task requires understanding the paragraph and question and then generating an answer, which naturally fits the encoder\'96decoder architecture of T5.\cf3 \strokec3 \
\pard\pardeftab720\sa280\partightenfactor0
\cf0 \strokec2 Comparison of architectures\
\pard\pardeftab720\sa240\partightenfactor0
\cf0 \strokec2 BERT (encoder-only):\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa240\partightenfactor0
\ls2\ilvl0\cf0 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Parameters to fine-tune: Very few (only a small output head)\
\ls2\ilvl0\kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Representation capacity: Excellent bidirectional understanding of text\
\ls2\ilvl0\kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Limitation: Cannot naturally generate answers; mainly suited for extractive QA\
\pard\pardeftab720\sa240\partightenfactor0
\cf0 \strokec2 GPT (decoder-only):\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa240\partightenfactor0
\ls3\ilvl0\cf0 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Parameters to fine-tune: Large (often most or all of the model)\
\ls3\ilvl0\kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Representation capacity: Very strong text generation\
\ls3\ilvl0\kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Limitation: Weaker bidirectional understanding of the input context\
\pard\pardeftab720\sa240\partightenfactor0
\cf0 \strokec2 T5 (encoder\'96decoder):\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa240\partightenfactor0
\ls4\ilvl0\cf0 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Parameters to fine-tune: Moderate (encoder + decoder)\
\ls4\ilvl0\kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Representation capacity: Strong balance of understanding and generation\
\ls4\ilvl0\kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Advantage: Most suitable for question-answer generation tasks\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 \
\pard\pardeftab720\sa280\partightenfactor0
\cf0 \strokec2 Using all architectures together\
\pard\pardeftab720\sa240\partightenfactor0
\cf0 \strokec2 Yes, we can combine them:\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa240\partightenfactor0
\ls5\ilvl0\cf0 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 BERT for understanding or retrieving relevant context\
\ls5\ilvl0\kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 GPT for fluent answer generation\
\ls5\ilvl0\kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 T5 to unify both understanding and generation in a single model\uc0\u8232 For example, a retrieval-augmented generation (RAG) system can use BERT for retrieval and T5 or GPT for generation.\kerning1\expnd0\expndtw0 \outl0\strokewidth0 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
<<< QUESTION 3 >>>\
\
\
Show that the self-attention score in the rotatory embeddings depends on the positional difference between the query and key.\
\
\
### WRITE ANSWER BELOW ###\
\
\pard\pardeftab720\sa240\partightenfactor0
\cf0 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 In rotary positional embeddings, positional information is added by rotating the query and key vectors based on their positions.\
If a query is at position p and a key is at position q, their embeddings are rotated using position-dependent rotation matrices. When we compute the attention score (dot product), the rotations combine in such a way that the result depends on q\uc0\u8722 p, the relative positional difference, rather than the absolute positions.\
Hence, the self-attention score in RoPE depends only on the relative distance between the query and key, which allows the model to naturally capture relative positional information.\
}