{\rtf1\ansi\ansicpg1252\cocoartf2822
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;}
{\*\expandedcolortbl;;\cssrgb\c0\c0\c0;}
{\*\listtable{\list\listtemplateid1\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid1\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid1}
{\list\listtemplateid2\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid101\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid2}
{\list\listtemplateid3\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid201\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid3}
{\list\listtemplateid4\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid301\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid4}
{\list\listtemplateid5\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid401\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid5}
{\list\listtemplateid6\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid501\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid6}}
{\*\listoverridetable{\listoverride\listid1\listoverridecount0\ls1}{\listoverride\listid2\listoverridecount0\ls2}{\listoverride\listid3\listoverridecount0\ls3}{\listoverride\listid4\listoverridecount0\ls4}{\listoverride\listid5\listoverridecount0\ls5}{\listoverride\listid6\listoverridecount0\ls6}}
\paperw11900\paperh16840\margl1440\margr1440\vieww12720\viewh7800\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 ------------------------------------------------------------------------------------------\
\
\
COURSE: Advanced AI\
TOPIC: Evolution of AI & Introduction to Transformer Models\
DUE-DATE: 19 Jan 2025\
\
\
INSTRUCTIONS:\
1. Do not modify any tag in this file. Just type your answers in the designated place.\
2. Do not change the file name and/or extension.\
3. Upload this file after answering in the portal.\
\
\
------------------------------------------------------------------------------------------\
\
\
<<< QUESTION 1 >>>\
\
\
What is Turing test? How is it performed? \
\
\
### WRITE ANSWER BELOW ###\
\
The Turing Test is a method in artificial intelligence used to determine whether a computer or machine can exhibit intelligence comparable to that of a human.\
\
The test involves three participants:\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\sl168\slmult1\pardirnatural\partightenfactor0
\cf0 \
i. An Interrogator (a human acting as a judge),\
\
ii. A Human, and\
\
iii. A Computer or Machine.\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
The interrogator asks a series of questions and receives responses from both the human and the machine without knowing their identities. Based solely on the responses, the interrogator then determines whether each answer was provided by a human or a machine.\
\
If the machine\'92s responses are indistinguishable from the human\'92s, it is considered to have passed the Turing Test.\
\
\
<<< QUESTION 2 >>>\
\
\
What is an expert system? Explain its characteristics and objectives.\
\
\
### WRITE ANSWER BELOW ###\
\
An expert system is a computer program that is designed to solve complex problems and to provide decision-making ability like a human expert in a specific domain. It performs this by extracting knowledge from its knowledge base using the reasoning and inference rules according to the user queries asked through user interface. \
\
Characteristics:\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls1\ilvl0\cf0 {\listtext	\uc0\u8226 	}Knowledge base - It contains KB that stores information, facts, riles and heuristics obtained form human experts in specific domain\
{\listtext	\uc0\u8226 	}Inference Engine - Engine of rules. Use KB to make decisions and draw consolations through logical reigning and inference mechanism.\
{\listtext	\uc0\u8226 	}Rule based reasoning - Operates on set of rules that encodes the expertise of human specialists, guiding the system\'92s decision-making process.\
{\listtext	\uc0\u8226 	}Knowledge acquisition & updating - Involves the systematic process of gathering & traversing expert knowledge to the system, often through document overview, reviews, observations, interviews etc. Allows for periodic updating of kb to incorporate new info and maintain relevance. Aims to assist users in decision-making within specified domain by providing recommendations, solutions , insights etc. \
{\listtext	\uc0\u8226 	}User Interface - provides user friendly interface to interact with expert system allowing users to input queries & receives responses.\
{\listtext	\uc0\u8226 	}Domain Specificity - expert systems designed for a specific and well-defined knowledge domain, focusing on areas where human expertise is crucial. \
{\listtext	\uc0\u8226 	}Reliable & High performance\
{\listtext	\uc0\u8226 	}Understandability & responsiveness\
{\listtext	\uc0\u8226 	}Goal oriented\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
Objectives:\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls2\ilvl0\cf0 {\listtext	\uc0\u8226 	}To transfer expertise from human experts to a computer system and \
{\listtext	\uc0\u8226 	}Then on to non-experts\
{\listtext	\uc0\u8226 	}Decision Support\
{\listtext	\uc0\u8226 	}Improved consistency, Training and education.\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
<<< QUESTION 3 >>>\
\
\
Why do modern generative models sometimes produce hallucinations?\
\
\
### WRITE ANSWER BELOW ###\
\
\pard\pardeftab720\sa240\partightenfactor0
\cf0 \expnd0\expndtw0\kerning0
Hallucinations in generative AI refer to outputs that are factually incorrect, contradictory, irrelevant, or nonsensical, even though they may sound fluent.\
Why do LLMs hallucinate?\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls3\ilvl0\cf0 \kerning1\expnd0\expndtw0 {\listtext	1	}\expnd0\expndtw0\kerning0
Training data quality: Models are trained on large, imperfect datasets (e.g., web sources), which may contain errors or inconsistencies. LLMs learn patterns, not verified facts.\
\ls3\ilvl0\kerning1\expnd0\expndtw0 {\listtext	2	}\expnd0\expndtw0\kerning0
Generation mechanism: LLMs generate text using softmax-based probability distributions and decoding strategies (sampling, beam search). These favor fluency and likelihood over factual correctness.\
\ls3\ilvl0\kerning1\expnd0\expndtw0 {\listtext	3	}\expnd0\expndtw0\kerning0
Lack of reasoning or verification: Unlike humans (or evaluations like the Turing Test), models don\'92t truly \'93understand\'94 or validate truth.\
\ls3\ilvl0\kerning1\expnd0\expndtw0 {\listtext	4	}\expnd0\expndtw0\kerning0
Prompt/context issues: Ambiguous, incomplete, or contradictory prompts can mislead the model, causing it to fill gaps with incorrect information.\kerning1\expnd0\expndtw0 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
<<< QUESTION 4 >>>\
\
\
What are the building blocks of encoder in transformer and what roles do they play? \
\
\
### WRITE ANSWER BELOW ###\
\
\pard\pardeftab720\partightenfactor0
\cf0 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 A Transformer decoder is mainly built using masked self-attention, feed-forward layers, and output prediction layers.\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls4\ilvl0\cf0 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8259 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Masked self-attention helps the model look only at previously generated words.\
\ls4\ilvl0\kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8259 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Attention layers allow the decoder to focus on important context.\
\ls4\ilvl0\kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8259 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Feed-forward layers process this information to learn patterns.\
\ls4\ilvl0\kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8259 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Finally, a linear layer with softmax predicts the next word in the sequence.\kerning1\expnd0\expndtw0 \outl0\strokewidth0 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
<<< QUESTION 5 >>>\
\
\
Explain multi-head attention and its advantages over single-head attention.\
\
\
### WRITE ANSWER BELOW ###\
\
\
\pard\pardeftab720\sa240\partightenfactor0
\cf0 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Multi-head attention is an extension of the attention mechanism where the model uses multiple attention heads in parallel instead of just one.\
Each head learns to focus on different parts or relationships in the input sequence (e.g., syntax, semantics, long-range dependencies). The outputs of all heads are then combined to form the final representation.\
Advantages over single-head attention:\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls5\ilvl0\cf0 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8259 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Captures multiple types of relationships simultaneously\
\ls5\ilvl0\kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8259 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Improves understanding of context and dependencies\
\ls5\ilvl0\kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8259 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 More expressive and robust representations\
\ls5\ilvl0\kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8259 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Performs better on complex tasks like NLP and vision\kerning1\expnd0\expndtw0 \outl0\strokewidth0 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
<<< QUESTION 6 >>>\
\
\
What is self-attention? Explain how does it work using Query, Key, and Value. \
\
\
### WRITE ANSWER BELOW ###\
\
\pard\pardeftab720\sa240\partightenfactor0
\cf0 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Self-attention is a mechanism that allows a model to understand a sequence by relating each token to every other token in the same sequence.\
How it works (Query, Key, Value):\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls6\ilvl0\cf0 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Each token is converted into three vectors: Query (Q), Key (K), and Value (V).\
\ls6\ilvl0\kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 The Query of one token is compared with the Keys of all tokens to calculate attention scores.\
\ls6\ilvl0\kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 These scores are scaled, passed through softmax, and converted into weights.\
\ls6\ilvl0\kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 The weights are applied to the Values to produce a weighted sum.\
\ls6\ilvl0\kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 This weighted sum becomes the token\'92s new representation, capturing relevant context.\
\pard\pardeftab720\sa240\partightenfactor0
\cf0 In short, self-attention lets each word focus on the most important words in the sequence.}