------------------------------------------------------------------------------------------


COURSE: Advanced AI
TOPIC: RAG
DUE-DATE: 15 Feb 2025


INSTRUCTIONS:
1. Do not modify any tag in this file. Just type your answers in the designated place.
2. Do not change the file name and/or extension.
3. Upload this file after answering in the portal.


------------------------------------------------------------------------------------------


<<< QUESTION 1 >>>


For the query “car premium insurance”, what will be the length-normalized tf-idf score for the word "car"? Assume that the corpus contains 1 million vocabulary terms, and the document frequency for {car, premium, insurance} is {100, 100, 10000}, respectively.


### WRITE ANSWER BELOW ###

For the query “car premium insurance”, each word appears once, so the term frequency of “car” is 1.
The total number of documents in the corpus is 1,000,000, and the document frequency of “car” is 100.

The inverse document frequency (IDF) of “car” is:

IDF(car) = log(1,000,000 / 100) = log(10,000)

Similarly:

IDF(premium) = log(10,000)

IDF(insurance) = log(100)

Thus, the TF-IDF values are:

car = log(10,000)

premium = log(10,000)

insurance = log(100)

Similarly, the IDF values of “premium” and “insurance” are log(10,000) and log(100) respectively.

The length of the query vector is calculated as: sqrt[(log(10000))^2 + (log(10000))^2 + (log(100))^2]

After computing TF-IDF for all query terms and normalizing by the length of the query vector, the length-normalized TF-IDF score for the word “car” is approximately 0.67.


<<< QUESTION 2 >>>



Given: A dataset of 1 million vectors (N = 10⁶).
Dimension (D): 128 (Float32, 4 bytes/float).
PQ Setup: Split vectors into m = 8 sub-vectors.
Codebook: Each sub-space uses k = 256* centroids.
Calculate:
1. Memory for Raw Storage (Naive k-NN).
2. Memory for Compressed Vectors (PQ indices).
3. Memory Overhead for the Codebooks (centroids).
4. The final Compression Ratio between k-NN and PQ. (Mem_k_NN / Mem_PQ)


### WRITE ANSWER BELOW ###

Given:
Number of vectors: N = 10 power of 6
Dimension D = 128
Each float = 4 bytes
Product Quantization (PQ) with: m = 8 sub-vectors, k=256 centroids per sub-space

1. Memory for Raw Storage (Naive k-NN)
In a naive k-NN approach, we store the full, uncompressed vectors.
Formula: N * D * (Bytes per float)
Calculation: 1,000,000 * 128 * 4 bytes
Result: 512,000,000 bytes
Memory (k-NN) = 512 MB

2. Memory for Compressed Vectors (PQ Indices)
With PQ, each vector is replaced by a set of "m" indices. Since the codebook size (k) is 256, each index can be represented by exactly 8 bits (1 byte), because 2 power of 8 = 256.
Formula: N * m * (Bytes per index)
Calculation: 1,000,000 * 8 * 1 byte
Result: 8,000,000 bytes
Memory (PQ indices) = 8 MB

3. Memory Overhead for the Codebooks (Centroids)
We must also store the centroids (the codebook) for each subspace to reconstruct or compare the vectors.
Each centroid has: Sub-vector dimension (d_sub): D / m = 128 / 8 = 16
Formula: m * k * d_sub * (Bytes per float)
Memory per centroid: Calculation: 8 * 256 * 16 * 4 bytes
For 8 sub-spaces: Result: 8×16,384=131,072 bytes ≈ 0.131 MB
Memory (Codebooks) ≈ ~0.131 MB

4. Final Compression Ratio
The compression ratio is the size of the original storage divided by the total PQ storage (compressed vectors + codebook overhead).

Total PQ Memory: 8,000,000 + 131,072 = 8,131,072 bytes
Compression Ratio: 512,000,000 / 8,131,072
Result: ~62.97


